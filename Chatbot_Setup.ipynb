{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fase 1: Setup (da svolgere entro Lunedì 13 Ottobre)"
      ],
      "metadata": {
        "id": "KXsRPAmSscOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installazione dei pacchetti Python utili\n",
        "La fase d'avvio di un notebook richiede l'installazione di tutti i pacchetti utili per eseguire le celle. Di conseguenza, come primo passo, lanciate la cella di codice sottostante per l'installazione."
      ],
      "metadata": {
        "id": "fQEL7JxGUM2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install unstructured\n",
        "!pip install dropbox\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y264MtJWRCbk",
        "outputId": "6f2109de-ce3a-43e2-969a-9a57ea8b589e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.331-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
            "  Downloading langsmith-0.0.58-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.331 langsmith-0.0.58 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.10.28-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.1)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.23.5)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=71c3bcc763db24b9fa92c84e3147c6ec4e0670c80fda50926b5ce56f09eda2f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, langdetect, emoji, backoff, unstructured\n",
            "Successfully installed backoff-2.2.1 emoji-2.8.0 filetype-1.2.0 langdetect-1.0.9 python-iso639-2023.6.15 python-magic-0.4.27 rapidfuzz-3.5.2 unstructured-0.10.28\n",
            "Collecting dropbox\n",
            "  Downloading dropbox-11.36.2-py3-none-any.whl (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.0/594.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.16.2 in /usr/local/lib/python3.10/dist-packages (from dropbox) (2.31.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from dropbox) (1.16.0)\n",
            "Collecting stone>=2 (from dropbox)\n",
            "  Downloading stone-3.3.1-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2023.7.22)\n",
            "Collecting ply>=3.4 (from stone>=2->dropbox)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ply, stone, dropbox\n",
            "Successfully installed dropbox-11.36.2 ply-3.11 stone-3.3.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definizione del gruppo di lavoro\n",
        "Steps:\n",
        "1. Formate una squadra di due persone con cui lavorare sulla creazione del chatbot.\n",
        "2. Inserire i nomi, le matricole dei membri e il turno di laboratorio della squadra nel seguente Foglio Google nelle colonne **Cognome e Nome 1**, **Numero Matricola 1**, **Cognome e Nome 2**, **Numero Matricola 2**, **Turno laborarorio**: https://docs.google.com/spreadsheets/d/1wOVeXRGP2TZHfQfmmWuienuoLVdb2OuxK6EfQCgMsNI/edit?usp=sharing\n"
      ],
      "metadata": {
        "id": "6G7zXJ0zr9rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creazione del Chatbot con Telegram\n",
        "\n",
        "Steps:\n",
        "1. Creazione di un bot su Telegram usando https://t.me/botfather; in questa fase scegliere un nome che sia rappresentativo dei documenti che il vostro canale usa per rispondere alle domande\n",
        "2. Salvarsi il token fornito al momento della creazione del bot\n",
        "3. Riempire le seguenti colonne nel Foglio Google:\n",
        "  *   Nome del chatbot: inserire il nome del chatbot\n",
        "  *   Link al chatbot: inserire il link al chatbot ottenuto al momento della creazione\n",
        "  * Descrizione del chatbot: inserire la stessa descrizione che inserite su Telegram\n"
      ],
      "metadata": {
        "id": "WvB9eVlJmvXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definire i documenti\n",
        "\n",
        "Il chatbot dovrà utilizzare come base di conoscenza una serie di documenti associati al tema scelto. I documenti dovranno essere minimo 3 e massimo 6, tra cui almeno un PDF ed una pagina web."
      ],
      "metadata": {
        "id": "KFccr-EJ7haG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pagine web\n",
        "\n",
        "Per le pagine web è sufficiente controllare che vengano caricate correttamente dalla libreria langchain di Python, che vedrete meglio nella seconda fase del laboratorio.\n",
        "Per il momento **SOSTITUITE I LINK DELLE PAGINE SCELTE DA VOI A QUELLI DELL'ESEMPIO SOTTOSTANTE VERIFICANDO CHE IL CODICE FUNZIONI CORRETTAMENTE!!!**\n",
        "\n",
        "Non è detto che tutte le pagine web funzionino correttamente o può essere che l'header delle pagine sia lungo e non venga rimosso. Se riscontraste questo tipo di problemi potete o cambiare la/e pagina/e web scelta/e o cercare soluzioni alternative, eventualmente consultando il seguente link: https://python.langchain.com/docs/modules/data_connection/document_loaders/html\n",
        "\n"
      ],
      "metadata": {
        "id": "555UuWxtQfih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa il modulo UnstructuredURLLoader dalla libreria langchain.document_loaders\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "# Definisci una lista di URL da cui scaricare contenuto\n",
        "urls = [\n",
        "    \"https://ourworldindata.org/artificial-intelligence\",\n",
        "    \"https://ourworldindata.org/causes-of-death\",\n",
        "    \"https://ourworldindata.org/terrorism\"\n",
        "]\n",
        "\n",
        "# Crea un oggetto 'loader' di tipo UnstructuredURLLoader, specificando gli URL da caricare\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "\n",
        "# Carica il contenuto dalle URL specificate\n",
        "documents = loader.load()\n",
        "\n",
        "# Loop attraverso ciascun documento ottenuto dalle URL\n",
        "for doc in documents:\n",
        "    # Estrai il contenuto della pagina\n",
        "    page_content = doc.page_content\n",
        "\n",
        "    # Ottieni la fonte del documento dalla metadati\n",
        "    source = doc.metadata[\"source\"]\n",
        "\n",
        "    # Estrai le prime 100 parole dal contenuto della pagina\n",
        "    first_words = \" \".join(page_content.split()[:100])\n",
        "\n",
        "    # Stampa le prime 100 parole della pagina insieme alla sua fonte\n",
        "    print(f\"Prime 100 parole della pagina {source}:\\n{first_words}\")\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zuY-W9B7f1q",
        "outputId": "d18662bb-d908-4b78-f54b-da23c7eed178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 100 words of the page https://ourworldindata.org/artificial-intelligence:\n",
            "Our World in Data Browse by topic Latest Resources About Subscribe Donate Gdoc Admin Artificial Intelligence By Charlie Giattino, Edouard Mathieu, Veronika Samborska and Max Roser Introduction Key Insights Research & Writing Charts Endnotes Cite This Work Reuse This Work Artificial intelligence (AI) systems already greatly impact our lives — they increasingly shape what we see, believe, and do. Based on the steady advances in AI technology and the significant recent increases in investment, we should expect AI technology to become even more powerful and impactful in the following years and decades. It is easy to underestimate how much the\n",
            "\n",
            "\n",
            "\n",
            "First 100 words of the page https://ourworldindata.org/causes-of-death:\n",
            "Our World in Data Browse by topic Latest Resources About Subscribe Donate Gdoc Admin Causes of Death By Saloni Dattani, Fiona Spooner, Hannah Ritchie and Max Roser Introduction Key Insights Research & Writing Charts Endnotes Cite This Work Reuse This Work What are people dying from? This question is essential to guide decisions in public health, and find ways to save lives. Many leading causes of death receive little mainstream attention. If news reports reflected what children died from, they would say that around 1,400 young children die from diarrheal diseases, 1,000 die from malaria, and 1,900 from respiratory infections\n",
            "\n",
            "\n",
            "\n",
            "First 100 words of the page https://ourworldindata.org/terrorism:\n",
            "Our World in Data Browse by topic Latest Resources About Subscribe Donate Gdoc Admin Terrorism By Bastian Herre, Veronika Samborska, Hannah Ritchie, Joe Hasell, Edouard Mathieu and Max Roser Introduction Key Insights Research & Writing Charts Endnotes Cite This Work Reuse This Work Terrorism is the threat or use of violence to intimidate or coerce in the pursuit of political or ideological goals. It is usually understood to be done by non-state actors — individuals or organizations not part of the government. Terrorism can take many forms, including bombings, armed assaults, hijackings, or hostage-taking. Its targets can also vary and\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF\n",
        "Per caricare il contenuto dei file PDF è invece necessario salvare i file su Github, quindi:\n",
        "\n",
        "1. Creare un account Github\n",
        "\n",
        "2. Creare un repository Github con la stessa struttura di questo: https://github.com/Loricanal/files_for_chatbot\n",
        "\n",
        "3. Creare una sottocartella chiamata PDF\n",
        "\n",
        "4. Caricare i file nella sottocartella su Github\n",
        "\n",
        "Per qualsiasi problema inerente questa fase, potete guardare il seguente tutorial: https://youtu.be/0cTmp5Vq1BM\n",
        "\n",
        "Successivamente servirà importare i documenti nel notebook, quindi:\n",
        "\n",
        "1. Clonare il repository\n",
        "\n",
        "2. Importare e leggere i file con la langchain (https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)\n",
        "\n",
        "Sotto è riportato il codice che esegue questi passaggi indicati.\n",
        "\n",
        "**AGGIORNATE IL CODICE CON IL VOSTRO REPOSITORY E IL VOSTRO CASO D'USO!!!**"
      ],
      "metadata": {
        "id": "zlmWPsjnURGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copia in locale del repository\n",
        "!git clone https://github.com/canacchia/files_for_chatbot.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uuT0zBAgOMq",
        "outputId": "7af34196-bc59-43c4-a049-63f984427777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'files_for_chatbot'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 14 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (14/14), 2.74 MiB | 4.96 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Controllo che tutti i file siano presenti\n",
        "!ls files_for_chatbot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi4UoMl7WPtF",
        "outputId": "b2d26e91-ef24-4cf6-84a0-f8708abd77ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Domande chatbot.csv'   PDF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importazione delle librerie PyPDFLoader e os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# Definisci il percorso alla cartella contenente i file PDF da elaborare\n",
        "path_to_PDF_files = \"files_for_chatbot/PDF/\"\n",
        "# Ottieni la lista di tutti i file nella cartella specificata\n",
        "files = os.listdir(path_to_PDF_files)\n",
        "\n",
        "# Crea una lista vuota chiamata 'documents' per contenere i documenti estratti dai file PDF\n",
        "documents = list()\n",
        "\n",
        "# Loop attraverso ogni file nella lista 'files'\n",
        "for f in files:\n",
        "    # Crea un oggetto 'loader' per PyPDFLoader specifico per il file PDF corrente\n",
        "    loader = PyPDFLoader(path_to_PDF_files + f)\n",
        "    # Carica e suddivide il contenuto del file PDF corrente\n",
        "    documents += loader.load_and_split()\n",
        "\n",
        "#Ho settato solo 3 documenti da visualizzare siccome il file caricato è un PDF con almeno 200 pagine e ogni pagina viene salvata come un documento\n",
        "# Per visualizzare solo i primi 3 documenti (pagine) dal PDF\n",
        "for doc in documents[:3]:\n",
        "    # Estrai il contenuto della pagina\n",
        "    page_content = doc.page_content\n",
        "\n",
        "    # Ottieni la fonte del documento dalla metadati\n",
        "    source = doc.metadata[\"source\"]\n",
        "\n",
        "    # Estrai le prime 100 parole dal contenuto della pagina\n",
        "    first_words = \" \".join(page_content.split()[:100])\n",
        "\n",
        "    # Stampa le prime 100 parole della pagina insieme alla sua fonte\n",
        "    print(f\"Prime 100 parole della pagina {source}:\\n{first_words}\")\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXFS6tPJXrHE",
        "outputId": "7d2709b0-e7c5-4799-d3f8-1b4d3f97e462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 100 words of the page files_for_chatbot/PDF/Giacomo Moro Mauretto - Se pianto un albero posso mangiare una bistecca_ Guida scientifica per un ambientalismo consapevole (2023, Mondadori).pdf:\n",
            "EIl libro metto più CO2 se mangio carne bovina biologica, polli da allevamento intensivo, frutti di mare o un trancio di pesce spada? È davvero utile adottare alveari di api o ﬁnanziare aziende o associazioni che piantano alberi in giro per il mondo? Ogni giorno la nostra vita è riempita da una serie di dilemmi etici e morali che assomigliano molto a queste domande. Ormai, infatti, abbiamo tutti gli strumenti per capire che ogni nostro comportamento produce un impatto sul pianeta. Eppure, nonostante questa consapevolezza, spesso ci sentiamo impotenti di fronte ai mille problemi che la crisi ambientale ci pone\n",
            "\n",
            "\n",
            "\n",
            "First 100 words of the page files_for_chatbot/PDF/Giacomo Moro Mauretto - Se pianto un albero posso mangiare una bistecca_ Guida scientifica per un ambientalismo consapevole (2023, Mondadori).pdf:\n",
            "L’autore Giacomo Moro Mauretto, in arte Entropy for Life, è laureato in Biologia Evoluzionistica. Negli ultimi anni, attraverso i suoi canali social, si è dedicato alla divulgazione scientiﬁca. Ogni giorno, con i suoi video, racconta con chiarezza e un rigoroso approccio scientiﬁco di animali, piante, evoluzione e questioni ambientali.\n",
            "\n",
            "\n",
            "\n",
            "First 100 words of the page files_for_chatbot/PDF/Giacomo Moro Mauretto - Se pianto un albero posso mangiare una bistecca_ Guida scientifica per un ambientalismo consapevole (2023, Mondadori).pdf:\n",
            "Giacomo Moro Mauretto SE PIANTO UN ALBERO POSSO MANGIARE UNA BISTECCA? Guida scientiﬁca per un ambientalismo consapevole\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definzione delle domande e delle risposte\n",
        "\n",
        "Per testare il vostro chatbot, una volta creato, sarà necessario valutare la sua capacità di fornire risposte corrette. In questa fase dovete quindi creare una lista di 10 domande a cui il chatbot dovrà rispondere. Oltre alle domande dovrete fornire le risposte corrette. Fate attenzione che nei documenti scelti ci sia l'informazione utile a rispondere alle domande.\n",
        "\n",
        "Uilizzare il seguente template per scrivere domande e risposte:\n",
        "https://docs.google.com/spreadsheets/d/13HtUubL-CaXFrNaym8HzQ_KSOmqpZIMdybLqjVW1at4/edit#gid=0\n",
        "\n",
        "A partire da esso:\n",
        "1. create una copia\n",
        "2. scrivete le vostre domande e risposte\n",
        "3. scaricatelo come file csv\n",
        "4. aggiungete il file csv al repository Github\n",
        "\n"
      ],
      "metadata": {
        "id": "_Cl9RlSp6rl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indicazioni per la sottomissione\n",
        "\n",
        "1. Una volta svolte tutte le richieste indicate nel notebook, scaricarlo (File > Scarica). Il formato finale deve essere \".ipynb\".\n",
        "2. Creare una cartella nominata con il nome del vostro bot; lo stesso indicato nel Foglio Google.\n",
        "3. Comprimere la cartella.\n",
        "4. Caricarla nella sezione Elaborati sul Portale della Didattica entro la data di consegna.\n",
        "\n"
      ],
      "metadata": {
        "id": "iKfWfze_crLC"
      }
    }
  ]
}